from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
from fuzzywuzzy import fuzz
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")

# Step 1: Load the Datasets
gpu_specs = pd.read_csv('gpu_specs_v6.csv')
benchmarks = pd.read_csv('GPU_scores_graphicsAPIs.csv')

# ---- DATASET 1: GPU SPECS ----
print("\n--- GPU Specs Dataset Overview ---")
print("Shape of GPU Specs Dataset:", gpu_specs.shape)
print("Columns in GPU Specs Dataset:", gpu_specs.columns.tolist())
print("First 5 rows of GPU Specs Dataset:\n", gpu_specs.head())
print("\nMissing Values in GPU Specs Dataset:\n", gpu_specs.isnull().sum())

# Step 2A: Clean GPU Specs Dataset
gpu_specs['manufacturer'] = gpu_specs['manufacturer'].str.strip().str.lower()
gpu_specs = gpu_specs[gpu_specs['manufacturer']
                      == 'nvidia']  # Filter for NVIDIA GPUs
gpu_specs.drop(columns=['manufacturer'], inplace=True)

# Handle missing values
numerical_columns = ['memSize', 'memBusWidth', 'gpuClock', 'memClock']
gpu_specs[numerical_columns] = gpu_specs[numerical_columns].fillna(
    gpu_specs[numerical_columns].median()
)
gpu_specs['memType'] = gpu_specs['memType'].fillna(
    gpu_specs['memType'].mode()[0])

# Print values used to fill missing data
print("\n--- Missing Value Handling in GPU Specs ---")
for col in numerical_columns:
    print(
        f"Filled missing values in {col} with median value: {gpu_specs[col].median()}")
print(
    f"Filled missing values in 'memType' with mode: {gpu_specs['memType'].mode()[0]}")


# Map memory types to simplified categories
mem_type_mapping = {
    'GDDR5': 'GDDR5', 'GDDR5X': 'GDDR5',
    'GDDR6': 'GDDR6', 'GDDR6X': 'GDDR6',
    'HBM2': 'HBM2', 'HBM': 'HBM'
    'DDR4': 'DDR4'
}
gpu_specs['memType'] = gpu_specs['memType'].map(
    mem_type_mapping).fillna('Other')

# Encode memory type for analysis
label_encoder_specs = LabelEncoder()
gpu_specs['memType_encoded'] = label_encoder_specs.fit_transform(
    gpu_specs['memType'])

# Feature engineering: Calculate memory bandwidth
gpu_specs['memoryBandwidth_GBs'] = (
    gpu_specs['memBusWidth'] * gpu_specs['memClock'] * 2 / 8
)

# ---- DATASET 2: BENCHMARKS ----
print("\n--- Benchmarks Dataset Overview ---")
print("Shape of Benchmarks Dataset:", benchmarks.shape)
print("Columns in Benchmarks Dataset:", benchmarks.columns.tolist())
print("First 5 rows of Benchmarks Dataset:\n", benchmarks.head())
print("\nMissing Values in Benchmarks Dataset:\n", benchmarks.isnull().sum())

# Step 2B: Clean Benchmarks Dataset
benchmarks['Manufacturer'] = benchmarks['Manufacturer'].str.strip().str.lower()
benchmarks = benchmarks[benchmarks['Manufacturer']
                        == 'nvidia']  # Filter for NVIDIA GPUs

# Fill missing CUDA and OpenCL values in benchmarks dataset
benchmarks['CUDA'] = benchmarks['CUDA'].fillna(benchmarks['CUDA'].median())
benchmarks['OpenCL'] = benchmarks['OpenCL'].fillna(
    benchmarks['OpenCL'].median())

# Print values used to fill missing data
print("\n--- Missing Value Handling in Benchmarks ---")
print(
    f"Filled missing values in 'CUDA' with median value: {benchmarks['CUDA'].median()}")
print(
    f"Filled missing values in 'OpenCL' with median value: {benchmarks['OpenCL'].median()}")

# ---- STEP 3: MATCH AND COMBINE DATASETS ----
# Fuzzy matching function
def fuzzy_match(row, choices, threshold=80):
    best_match = None
    best_score = 0
    for choice in choices:
        score = fuzz.token_sort_ratio(row, choice)
        if score > best_score and score >= threshold:
            best_score = score
            best_match = choice
    return best_match


# Extract unique device names
gpu_specs_products = gpu_specs['productName'].unique()
benchmarks_devices = benchmarks['Device'].unique()

# Perform fuzzy matching for product names
gpu_specs['matched_device'] = gpu_specs['productName'].apply(
    lambda x: fuzzy_match(x, benchmarks_devices)
)

# Select only relevant columns before merging
gpu_specs_relevant_columns = ['productName', 'memSize', 'memBusWidth',
                              'gpuClock', 'memClock', 'memType', 'memoryBandwidth_GBs', 'matched_device']
benchmarks_relevant_columns = ['Device', 'CUDA', 'OpenCL']

# Merge datasets on matched device names
combined_data = pd.merge(
    gpu_specs[gpu_specs_relevant_columns], benchmarks[benchmarks_relevant_columns],
    left_on='matched_device', right_on='Device', how='inner'
)

# Drop the 'matched_device' and 'Device' columns, if needed
combined_data.drop(columns=['matched_device', 'Device'], inplace=True)

# ---- OVERVIEW OF COMBINED DATASET ----
print("\n--- Combined Dataset Overview ---")
print("Shape of Combined Dataset:", combined_data.shape)
print("Columns in Combined Dataset:", combined_data.columns.tolist())
print("First 5 rows of Combined Dataset:\n", combined_data.head())
print("\nMissing Values in Combined Dataset:\n", combined_data.isnull().sum())


# Ensure memType exists in the final merged dataset
print("Columns in the merged dataset:", combined_data.columns)

# Save the final combined dataset to a CSV
combined_data.to_csv('combined_gpu_data_with_benchmarks2.csv', index=False)

# ---- STEP 4: EXPLORATORY DATA ANALYSIS ----
sns.set_theme(style="whitegrid")

# Enhanced: Distribution of Memory Size
plt.figure(figsize=(14, 8))  # Slightly larger for better readability

# Histogram with KDE
sns.histplot(
    combined_data['memSize'],
    bins=20,  # Smaller bin width for more granularity
    kde=True,
    color='skyblue',
    edgecolor='black',
    linewidth=1.2
)

# Title and Axes Labels
plt.title('Figure 1: Frequency Distribution of GPU Memory Sizes (in GB)',
          fontsize=20, pad=15)
plt.xlabel('Memory Size (GB)', fontsize=16, labelpad=10)
plt.ylabel('Count', fontsize=16, labelpad=10)

# X-Axis and Y-Axis Ticks
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Annotations for Key Insights
plt.axvline(x=10, color='red', linestyle='--', linewidth=1.5,
            label='Common Memory Limit (10 GB)')
plt.text(11, plt.ylim()[1]*0.8, 'Most devices <10 GB',
         color='red', fontsize=12)

# Highlighting outliers
plt.axvline(x=50, color='green', linestyle='--', linewidth=1.5,
            label='High-Capacity Devices (50+ GB)')
plt.text(51, plt.ylim()[1]*0.5, 'Outliers (50+ GB)',
         color='green', fontsize=12)

# Adding a Legend
plt.legend(fontsize=12, title='Annotations',
           title_fontsize=14, loc='upper right')

# Adding Grid for readability
plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)

# Tight Layout for cleaner spacing
plt.tight_layout()

# Display the plot
plt.show()


# Create a count plot by grouping by memType and memBusWidth
plt.figure(figsize=(12, 7))
sns.barplot(x='memBusWidth', hue='memType', data=gpu_specs, palette='viridis')
plt.title('Figure 2: Memory Bus Width Frequency by Memory Type',
          fontsize=18, fontweight='bold')
plt.xlabel('Memory Bus Width (bits)', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(fontsize=12, rotation=45)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

# Memory Type Distribution
plt.figure(figsize=(12, 7))
sns.countplot(data=gpu_specs, x='memType',
              palette='viridis', edgecolor='black')
plt.title('Figure 3: Distribution of Memory Types', fontsize=18, fontweight='bold')
plt.xlabel('Memory Type', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(fontsize=12, rotation=45)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

# List of unique memory types
memory_types = gpu_specs['memType'].unique()

# Initialize a figure counter
figure_counter = 4

# Set up the plot size
plt.figure(figsize=(12, 7))

# Loop through each memory type and plot a scatterplot for each
for mem_type in memory_types:
    # Filter the data for the specific memory type
    filtered_data = gpu_specs[gpu_specs['memType'] == mem_type]

    # Create a scatterplot for each memory type
    plt.figure(figsize=(12, 7))
    sns.scatterplot(data=filtered_data, x='memClock',
                    y='memSize', s=100, alpha=0.8)
    plt.title(
        f'Memory Clock Speed vs. Memory Size for {mem_type}', fontsize=18, fontweight='bold')
    plt.xlabel('Memory Clock Speed (MHz)', fontsize=14)
    plt.ylabel('Memory Size (GB)', fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
# Increment the figure counter
    figure_counter += 1

    plt.show()

# List of selected GPUs
selected_gpus = ['GeForce RTX 4090', 'GeForce RTX 3090', 'GeForce RTX 2080',
                 'GeForce GTX 1080', 'Tesla V100S PCIe 32 GB']  # Example GPU names

# Filter the combined dataset for the selected GPUs
filtered_data = combined_data[combined_data['productName'].isin(selected_gpus)]

# Select only the relevant columns for visualization (memory type, benchmark scores)
filtered_data = filtered_data[['productName', 'memSize', 'memType', 'CUDA', 'OpenCL']]

# Melt the filtered data for easier plotting (keep product name and memory type as id_vars)
filtered_data_melted = filtered_data.melt(
    id_vars=['productName', 'memSize', 'memType'], var_name='Benchmark', value_name='Score')

# Function to annotate memory size and memory type with adjusted vertical space
def annotate_gpu_info(ax, gpu_name, memory_size, memory_type, x_pos, y_pos):
    # Display the memory size above the first bar (CUDA) with an increased offset
    ax.text(x_pos, y_pos + 10000,  # Increased offset for memory size
            f"{memory_size} GB",  # Display memory size
            ha='center', fontsize=10, color='black', fontweight='bold')
    
    # Display the memory type slightly further above the memory size
    ax.text(x_pos, y_pos + 16000,  # Further increased offset for memory type
            f"{memory_type}",  # Display memory type
            ha='center', fontsize=10, color='black', fontweight='bold')

# Create a bar chart
plt.figure(figsize=(14, 8))  # Increase figure size to give more space

# Plot the data (memory size, memory type, and benchmark scores)
ax = sns.barplot(
    data=filtered_data_melted,
    x='productName',  # x-axis shows product names (GPUs)
    y='Score',        # y-axis shows the benchmark score
    hue='Benchmark',  # Differentiate between CUDA and OpenCL
    palette='coolwarm',  # Optional: color palette
    edgecolor='black'
)

# Add memory size and memory type annotations only once for each GPU
seen_gpus = set()  # Track GPUs we've already annotated
for i, row in filtered_data_melted.iterrows():
    gpu_name = row['productName']
    memory_size = row['memSize']
    memory_type = row['memType']
    y_pos = row['Score']
    
    # Only add annotations once for each GPU
    if gpu_name not in seen_gpus:
        # Get the x position for the first bar of each GPU (CUDA bar is always first)
        x_pos = filtered_data_melted[filtered_data_melted['productName'] == gpu_name].index[0]
        
        # Call the function to annotate both memory size and memory type with an increased gap
        annotate_gpu_info(ax, gpu_name, memory_size, memory_type, x_pos, y_pos)
        
        # Mark this GPU as annotated
        seen_gpus.add(gpu_name)

# Adjust y-axis limit to avoid overlap with title
max_score = filtered_data_melted['Score'].max()
ax.set_ylim(0, max_score + 30000)  # Give more space at the top (30000 is an arbitrary number)

# Adding titles and labels with additional padding
plt.title('Figure 9: Benchmark Scores for Selected GPUs (CUDA and OpenCL)', fontsize=18, fontweight='bold', pad=20)
plt.xlabel('GPU Name', fontsize=14)
plt.ylabel('Benchmark Score', fontsize=14)

# Adding grid for readability
plt.grid(visible=True, linestyle='--', alpha=0.6)

# Adjusting ticks and layout
plt.xticks(fontsize=12, rotation=45, ha='right')  # Rotate labels for better readability
plt.yticks(fontsize=12)
plt.tight_layout()

# Display the plot
plt.show()

# Example: Correlation Matrix
plt.figure(figsize=(10, 7))
correlation_matrix = combined_data[[
    'memSize', 'memBusWidth', 'gpuClock', 'memClock', 'memoryBandwidth_GBs']].corr()
sns.heatmap(correlation_matrix, annot=True,
            cmap='coolwarm', fmt='.2f', linewidths=1)
plt.title('Figure 10: Correlation Matrix for Numerical Features', fontsize=18)
plt.tight_layout()
plt.show()

# ----- Modelling ------
# ---- STEP 1: Data Preprocessing ----
# Define features (X) and target (y)
X = combined_data[['memSize', 'memBusWidth',
                   'gpuClock', 'memClock', 'memoryBandwidth_GBs']]
y = combined_data['CUDA']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---- STEP 2: Hyperparameter Tuning (Manually Selected) ----
best_params = {
    'learning_rate': 0.05,
    'max_depth': 5,
    'min_child_weight': 1,
    'subsample': 1.0,
    'colsample_bytree': 0.8,
    'n_estimators': 200,
    'gamma': 0.1
}

# Initialize the XGBRegressor with best parameters
best_xgb = XGBRegressor(
    learning_rate=best_params['learning_rate'],
    max_depth=best_params['max_depth'],
    min_child_weight=best_params['min_child_weight'],
    subsample=best_params['subsample'],
    colsample_bytree=best_params['colsample_bytree'],
    n_estimators=best_params['n_estimators'],
    gamma=best_params['gamma'],
    objective='reg:squarederror',
    random_state=42
)
# ---- STEP 3: Train XGBoost with xgb.train() and Early Stopping ----

# Convert the data into DMatrix format
dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
dtest = xgb.DMatrix(X_test_scaled, label=y_test)

# Set up the parameters for xgb.train()
params = {
    'learning_rate': best_params['learning_rate'],
    'max_depth': best_params['max_depth'],
    'min_child_weight': best_params['min_child_weight'],
    'subsample': best_params['subsample'],
    'colsample_bytree': best_params['colsample_bytree'],
    'gamma': best_params['gamma'],
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse'
}

# Specify the evaluation set
evals = [(dtest, 'eval'), (dtrain, 'train')]

# Train the model with early stopping
num_round = best_params['n_estimators']
early_stopping_rounds = 50  # Stop if validation score doesn't improve for 50 rounds
bst = xgb.train(
    params, dtrain, num_round, evals=evals,
    early_stopping_rounds=early_stopping_rounds,
    verbose_eval=True
)

